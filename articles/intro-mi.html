<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Introduction to Multiple Imputation • Amelia</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Introduction to Multiple Imputation">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">


    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">Amelia</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">1.8.3</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles

    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/intro-mi.html">Introduction to Multiple Imputation</a>
    </li>
    <li>
      <a href="../articles/using-amelia.html">Using Amelia</a>
    </li>
    <li>
      <a href="../articles/diagnostics.html">Multiple Imputation Diagnostics</a>
    </li>
    <li>
      <a href="../articles/ameliaview.html">AmeliaView GUI Guide</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->



      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Introduction to Multiple Imputation</h1>
            
            <h4 data-toc-skip class="date">2024-11-07</h4>
      

      <div class="hidden name"><code>intro-mi.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="sec:intro">Introduction<a class="anchor" aria-label="anchor" href="#sec:intro"></a>
</h2>
<p>Missing data is a ubiquitous problem in social science data.
Respondents do not answer every question, countries do not collect
statistics every year, archives are incomplete, subjects drop out of
panels. Most statistical analysis methods, however, assume the absence
of missing data, and are only able to include observations for which
every variable is measured. Amelia allows users to impute (“fill in” or
rectangularize) incomplete data sets so that analyses which require
complete observations can appropriately use all the information present
in a dataset with missingness, and avoid the biases, inefficiencies, and
incorrect uncertainty estimates that can result from dropping all
partially observed observations from the analysis.</p>
<p>Amelia performs <em>multiple imputation</em>, a general-purpose
approach to data with missing values. Multiple imputation has been shown
to reduce bias and increase efficiency compared to listwise deletion.
Furthermore, ad-hoc methods of imputation, such as mean imputation, can
lead to serious biases in variances and covariances. Unfortunately,
creating multiple imputations can be a burdensome process due to the
technical nature of algorithms involved.  provides users with a simple
way to create and implement an imputation model, generate imputed
datasets, and check its fit using diagnostics.</p>
<p>The Amelia program goes several significant steps beyond the
capabilities of the first version of Amelia <span class="citation">(<a href="#ref-HonJosKin98">Honaker et al. 1998-2002</a>)</span>. For one,
the bootstrap-based EMB algorithm included in Amelia can impute many
more variables, with many more observations, in much less time. The
great simplicity and power of the EMB algorithm made it possible to
write Amelia so that it virtually never crashes — which to our knowledge
makes it unique among all existing multiple imputation software — and is
much faster than the alternatives too. Amelia also has features to make
valid and much more accurate imputations for cross-sectional,
time-series, and time-series-cross-section data, and allows the
incorporation of observation and data-matrix-cell level prior
information. In addition to all of this, Amelia provides many diagnostic
functions that help users check the validity of their imputation model.
This software implements the ideas developed in <span class="citation">Honaker and King (<a href="#ref-HonKin10">2010</a>)</span>.</p>
</div>
<div class="section level2">
<h2 id="sec:what">What Amelia Does<a class="anchor" aria-label="anchor" href="#sec:what"></a>
</h2>
<p>Multiple imputation involves imputing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
values for each missing cell in your data matrix and creating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
“completed” data sets. Across these completed data sets, the observed
values are the same, but the missing values are filled in with a
distribution of imputations that reflect the uncertainty about the
missing data. After imputation with Amelia’s EMB algorithm, you can
apply whatever statistical method you would have used if there had been
no missing values to each of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
data sets, and use a simple procedure, described below, to combine the
results. Under normal circumstances, you only need to impute once and
can then analyze the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
imputed data sets as many times and for as many purposes as you wish.
The advantage of Amelia is that it combines the comparative speed and
ease-of-use of our algorithm with the power of multiple imputation, to
let you focus on your substantive research questions rather than
spending time developing complex application-specific models for
nonresponse in each new data set. Unless the rate of missingness is very
high,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">m = 5</annotation></semantics></math>
(the program default) is probably adequate.</p>
<div class="section level3">
<h3 id="assumptions">Assumptions<a class="anchor" aria-label="anchor" href="#assumptions"></a>
</h3>
<p>The imputation model in Amelia assumes that the complete data (that
is, both observed and unobserved) are multivariate normal. If we denote
the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>×</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(n \times k)</annotation></semantics></math>
dataset as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>
(with observed part
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msup><annotation encoding="application/x-tex">D^{obs}</annotation></semantics></math>
and unobserved part
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>s</mi></mrow></msup><annotation encoding="application/x-tex">D^{mis}</annotation></semantics></math>),
then this assumption is</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>∼</mo><msub><mi>𝒩</mi><mi>k</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>,</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation} 
D \sim \mathcal{N}_k(\mu, \Sigma), 
\end{equation}</annotation></semantics></math></p>
<p>which states that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>
has a multivariate normal distribution with mean vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math>
and covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>.
The multivariate normal distribution is often a crude approximation to
the true distribution of the data, yet there is evidence that this model
works as well as other, more complicated models even in the face of
categorical or mixed data <span class="citation">Schafer and Olsen (<a href="#ref-SchOls98">1998</a>)</span>. Furthermore, transformations of
many types of variables can often make this normality assumption more
plausible (see @ref(sec:trans) for more information on how to implement
this in Amelia).</p>
<p>The essential problem of imputation is that we only observe
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msup><annotation encoding="application/x-tex">D^{obs}</annotation></semantics></math>,
not the entirety of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>.
In order to gain traction, we need to make the usual assumption in
multiple imputation that the data are <em>missing at random</em> (MAR).
This assumption means that the pattern of missingness only depends on
the observed data
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msup><annotation encoding="application/x-tex">D^{obs}</annotation></semantics></math>,
not the unobserved data
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>s</mi></mrow></msup><annotation encoding="application/x-tex">D^{mis}</annotation></semantics></math>.
Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>
to be the missingness matrix, with cells
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">m_{ij} = 1</annotation></semantics></math>
if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>∈</mo><msup><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>s</mi></mrow></msup></mrow><annotation encoding="application/x-tex">d_{ij} \in D^{mis}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">m_{ij} = 0</annotation></semantics></math>
otherwise. Put simply,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>
is a matrix that indicates whether or not a cell is missing in the data.
With this, we can define the MAR assumption as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>M</mi><mo stretchy="false" form="prefix">|</mo><mi>D</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>M</mi><mo stretchy="false" form="prefix">|</mo><msup><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
 p(M|D) = p(M|D^{obs}).
</annotation></semantics></math></p>
<p>Note that MAR includes the case when missing values are created
randomly by, say, coin flips, but it also includes many more
sophisticated missingness models. When missingness is not dependent on
the data at all, we say that the data are <em>missing completely at
random</em> (MCAR). Amelia requires both the multivariate normality and
the MAR assumption (or the simpler special case of MCAR). Note that the
MAR assumption can be made more plausible by including additional
variables in the dataset
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>
in the imputation dataset than just those eventually envisioned to be
used in the analysis model.</p>
</div>
<div class="section level3">
<h3 id="algorithm">Algorithm<a class="anchor" aria-label="anchor" href="#algorithm"></a>
</h3>
<p>In multiple imputation, we are concerned with the complete-data
parameters,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>,</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\theta = (\mu, \Sigma)</annotation></semantics></math>.
When writing down a model of the data, it is clear that our observed
data is actually
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msup><annotation encoding="application/x-tex">D^{obs}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>,
the missingness matrix. Thus, the likelihood of our observed data is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msup><mo>,</mo><mi>M</mi><mo stretchy="false" form="prefix">|</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(D^{obs}, M|\theta)</annotation></semantics></math>.
Using the MAR assumption, we can break this up,</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msup><mo>,</mo><mi>M</mi><mo stretchy="false" form="prefix">|</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>M</mi><mo stretchy="false" form="prefix">|</mo><msup><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msup><mo stretchy="false" form="prefix">|</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
  p(D^{obs},M|\theta) = p(M|D^{obs})p(D^{obs}|\theta).
\end{align}</annotation></semantics></math></p>
<p>As we only care about inference on the complete data parameters, we
can write the likelihood as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="false" form="prefix">|</mo><msup><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>∝</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msup><mo stretchy="false" form="prefix">|</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
  L(\theta|D^{obs}) &amp;\propto p(D^{obs}|\theta),
\end{align}</annotation></semantics></math></p>
<p>which we can rewrite using the law of iterated expectations as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msup><mo stretchy="false" form="prefix">|</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mo>∫</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>D</mi><mo stretchy="false" form="prefix">|</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><msup><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>s</mi></mrow></msup><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
  p(D^{obs}|\theta) &amp;= \int p(D|\theta) dD^{mis}.
\end{align}</annotation></semantics></math></p>
<p>With this likelihood and a flat prior on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>,
we can see that the posterior is</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="false" form="prefix">|</mo><msup><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>∝</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msup><mo stretchy="false" form="prefix">|</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>∫</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>D</mi><mo stretchy="false" form="prefix">|</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><msup><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>s</mi></mrow></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation}
  p(\theta | D^{obs}) \propto p(D^{obs}|\theta) = \int p(D|\theta)
  dD^{mis}.
\end{equation}</annotation></semantics></math></p>
<p>The main computational difficulty in the analysis of incomplete data
is taking draws from this posterior. The EM algorithm <span class="citation">(<a href="#ref-DemLaiRub77">Dempster, Laird, and Rubin
1977</a>)</span> is a simple computational approach to finding the mode
of the posterior. Our EMB algorithm combines the classic EM algorithm
with a bootstrap approach to take draws from this posterior. For each
draw, we bootstrap the data to simulate estimation uncertainty and then
run the EM algorithm to find the mode of the posterior for the
bootstrapped data, which gives us fundamental uncertainty too <span class="citation">(see <a href="#ref-HonKin10">Honaker and King 2010</a>
for details of the EMB algorithm)</span>.</p>
<p>Once we have draws of the posterior of the complete-data parameters,
we make imputations by drawing values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>s</mi></mrow></msup><annotation encoding="application/x-tex">D^{mis}</annotation></semantics></math>
from its distribution conditional on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msup><annotation encoding="application/x-tex">D^{obs}</annotation></semantics></math>
and the draws of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>,
which is a linear regression with parameters that can be calculated
directly from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>.</p>
</div>
<div class="section level3">
<h3 id="analysis">Analysis<a class="anchor" aria-label="anchor" href="#analysis"></a>
</h3>
<p>In order to combine the results across
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
data sets, first decide on the quantity of interest to compute, such as
a univariate mean, regression coefficient, predicted probability, or
first difference. Then, the easiest way is to draw
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>/</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">1/m</annotation></semantics></math>
simulations of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics></math>
from each of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
data sets, combine them into one set of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
simulations, and then to use the standard simulation-based methods of
interpretation common for single data sets <span class="citation">King,
Tomz, and Wittenberg (<a href="#ref-KinTomWit00">2000</a>)</span>.</p>
<p>Alternatively, you can combine directly and use as the multiple
imputation estimate of this parameter,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>q</mi><mo accent="true">‾</mo></mover><annotation encoding="application/x-tex">\bar{q}</annotation></semantics></math>,
the average of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
separate estimates,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>q</mi><mi>j</mi></msub><annotation encoding="application/x-tex">q_j</annotation></semantics></math><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(j=1,\dots,m)</annotation></semantics></math>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>q</mi><mo accent="true">‾</mo></mover><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>q</mi><mi>j</mi></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation}
  \bar{q}=\frac{1}{m}\sum^{m}_{j=1}q_j.
\end{equation}</annotation></semantics></math></p>
<p>The variance of the point estimate is the average of the estimated
variances from <em>within</em> each completed data set, plus the sample
variance in the point estimates <em>across</em> the data sets
(multiplied by a factor that corrects for the bias because
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>&lt;</mo><mi>∞</mi></mrow><annotation encoding="application/x-tex">m&lt;\infty</annotation></semantics></math>).
Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>E</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>q</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">SE(q_j)^2</annotation></semantics></math>
denote the estimated variance (squared standard error) of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>q</mi><mi>j</mi></msub><annotation encoding="application/x-tex">q_j</annotation></semantics></math>
from the data set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>S</mi><mi>q</mi><mn>2</mn></msubsup><mo>=</mo><msubsup><mi>Σ</mi><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>q</mi><mi>j</mi></msub><mo>−</mo><mover><mi>q</mi><mo accent="true">‾</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">S^{2}_{q}=\Sigma^{m}_{j=1}(q_j-\bar{q})^2/(m-1)</annotation></semantics></math>
be the sample variance across the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
point estimates. The standard error of the multiple imputation point
estimate is the square root of</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>E</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>q</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>S</mi><mi>E</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>q</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>+</mo><msubsup><mi>S</mi><mi>q</mi><mn>2</mn></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mn>1</mn><mi>/</mi><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation}
SE(q)^2=\frac{1}{m}\sum^{m}_{j=1}SE(q_j)^2+S^2_q(1+1/m).
\end{equation}</annotation></semantics></math></p>
</div>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-DemLaiRub77" class="csl-entry">
Dempster, Arthur P., N. M. Laird, and D. B. Rubin. 1977. <span>“Maximum
Likelihood Estimation from Incomplete Data via the Em Algorithm.”</span>
<em>Journal of the Royal Statistical Society B</em> 39: 1–38.
</div>
<div id="ref-HonJosKin98" class="csl-entry">
Honaker, James, Anne Joseph, Gary King, Kenneth Scheve, and Naunihal
Singh. 1998-2002. <span>“: A Program for Missing Data.”</span>
</div>
<div id="ref-HonKin10" class="csl-entry">
Honaker, James, and Gary King. 2010. <span>“What to Do about Missing
Values in Time Series Cross-Section Data.”</span> <em>American Journal
of Political Science</em> 54 (2): 561–81.
</div>
<div id="ref-KinTomWit00" class="csl-entry">
King, Gary, Michael Tomz, and Jason Wittenberg. 2000. <span>“Making the
Most of Statistical Analyses: Improving Interpretation and
Presentation.”</span> <em>American Journal of Political Science</em> 44
(2): 341–55.
</div>
<div id="ref-Schafer97" class="csl-entry">
Schafer, Joseph L. 1997. <em>Analysis of Incomplete Multivariate
Data</em>. London: Chapman &amp; Hall.
</div>
<div id="ref-SchOls98" class="csl-entry">
Schafer, Joseph L., and Maren K. Olsen. 1998. <span>“Multiple Imputation
for Multivariate Missing-Data Problems: A Data Analyst’s
Perspective.”</span> <em>Multivariate Behavioral Research</em> 33 (4):
545–71.
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

      </div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by <a href="https://hona.kr" class="external-link">James Honaker</a>, <a href="https://gking.harvard.edu" class="external-link">Gary King</a>, <a href="https://www.mattblackwell.org" class="external-link">Matthew Blackwell</a>.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

      </footer>
</div>






  </body>
</html>
